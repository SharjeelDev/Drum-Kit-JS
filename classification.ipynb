{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engr-owais-ali/Drum-Kit-JS/blob/patch-2/classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDgF4OfztbHe"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmQ6nLWPtbHh"
      },
      "source": [
        "To run this script, you need the following files found in the /data directory:\n",
        "- \"final_labels_SG1.xlsx\"\n",
        "- \"final_labels_SG2.xlsx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4kqNVQx-pD4"
      },
      "source": [
        "## Imports and set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92Q3zPSjalf0",
        "outputId": "8c9205a3-9b15-4617-84a7-5571ac2871d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 74.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 19.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d2EFrDIUXUG",
        "outputId": "faca9dd3-75b0-45fe-fa33-9b3b14ade319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan  2 13:31:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "667g0q9jYX8D"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "from typing import Dict, List, Optional, Union\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, BertConfig, TFBertForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
        "from transformers import ElectraTokenizer, TFElectraForSequenceClassification\n",
        "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
        "from transformers import LongformerTokenizer, TFLongformerForSequenceClassification\n",
        "from transformers import BartTokenizer, BartForSequenceClassification, FlaxBartForSequenceClassification\n",
        "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification\n",
        "from transformers import ConvBertTokenizer, TFConvBertForSequenceClassification\n",
        "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
        "from transformers import DebertaTokenizer, TFDebertaForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "abdzY7ARYX8M"
      },
      "outputs": [],
      "source": [
        "# set seed, TF uses python ramdom and numpy library, so these must also be fixed\n",
        "tf.random.set_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "os.environ['PYTHONHASHSEED']=str(0)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkbVcSRQppUw",
        "outputId": "d7d8d0f8-17d8-48f0-d565-2a09d5cf97cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# see if hardware accelerator available\n",
        "tf.config.experimental.list_physical_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Pboq8uWHq2oe",
        "outputId": "378b4e51-f75d-4894-a0f1-1c1ba08bfa91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLsR1EXet7t1",
        "outputId": "50fa4b36-1d27-4775-af0e-91e605244cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OhcRZzzuIms",
        "outputId": "45904b7c-18b1-490e-ba30-58142ec5bb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github/Neural-Media-Bias-Detection-Using-Distant-Supervision-With-BABE\n",
            "annotation_guidelines_BABE.pdf\t     demographic_questionnaire.pdf\n",
            "annotator_demographics.csv\t     distant_supervision.ipynb\n",
            "checkpoints\t\t\t     features_engineering.ipynb\n",
            "classification_baseline_model.ipynb  LICENSE\n",
            "classification.ipynb\t\t     README.md\n",
            "data\t\t\t\t     topics_keywords_platforms.txt\n",
            "data_set_evaluation.ipynb\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/Github/Neural-Media-Bias-Detection-Using-Distant-Supervision-With-BABE\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlNn9cexsKPx"
      },
      "source": [
        "If GPUs are available, tensorflow will give priority to it automatically and computations will be performed on the GPU as default. That behavior can be changed by assigning a task explicitly to a device. Example:\n",
        "\n",
        "```\n",
        "with tf.device('/CPU:0'):\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-sm3aAxZr5R"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "k3AlX8qqroZE",
        "outputId": "71cf3c57-5e9a-422c-d031-995bef557ce8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  The Republican president assumed he was helpin...   \n",
              "1  Though the indictment of a woman for her own p...   \n",
              "2  Ingraham began the exchange by noting American...   \n",
              "3  The tragedy of America’s 18 years in Afghanist...   \n",
              "4  The justices threw out a challenge from gun ri...   \n",
              "\n",
              "                                           news_link     outlet  \\\n",
              "0  http://www.msnbc.com/rachel-maddow-show/auto-i...      msnbc   \n",
              "1  https://eu.usatoday.com/story/news/nation/2019...  usa-today   \n",
              "2  https://www.breitbart.com/economy/2020/01/12/d...  breitbart   \n",
              "3  http://feedproxy.google.com/~r/breitbart/~3/ER...  breitbart   \n",
              "4  https://www.huffpost.com/entry/supreme-court-g...      msnbc   \n",
              "\n",
              "                                   topic    type    Label_bias  \\\n",
              "0                            environment    left        Biased   \n",
              "1                               abortion  center    Non-biased   \n",
              "2                            immigration   right  No agreement   \n",
              "3  international-politics-and-world-news   right        Biased   \n",
              "4                            gun-control    left    Non-biased   \n",
              "\n",
              "                           label_opinion             biased_words  \n",
              "0             Expresses writer’s opinion                       []  \n",
              "1  Somewhat factual but also opinionated                       []  \n",
              "2                           No agreement                ['flood']  \n",
              "3  Somewhat factual but also opinionated  ['tragedy', 'stubborn']  \n",
              "4                       Entirely factual                       []  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d02bc569-9c1e-435e-9179-c7e0e8f69b9f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>news_link</th>\n",
              "      <th>outlet</th>\n",
              "      <th>topic</th>\n",
              "      <th>type</th>\n",
              "      <th>Label_bias</th>\n",
              "      <th>label_opinion</th>\n",
              "      <th>biased_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Republican president assumed he was helpin...</td>\n",
              "      <td>http://www.msnbc.com/rachel-maddow-show/auto-i...</td>\n",
              "      <td>msnbc</td>\n",
              "      <td>environment</td>\n",
              "      <td>left</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Expresses writer’s opinion</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Though the indictment of a woman for her own p...</td>\n",
              "      <td>https://eu.usatoday.com/story/news/nation/2019...</td>\n",
              "      <td>usa-today</td>\n",
              "      <td>abortion</td>\n",
              "      <td>center</td>\n",
              "      <td>Non-biased</td>\n",
              "      <td>Somewhat factual but also opinionated</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ingraham began the exchange by noting American...</td>\n",
              "      <td>https://www.breitbart.com/economy/2020/01/12/d...</td>\n",
              "      <td>breitbart</td>\n",
              "      <td>immigration</td>\n",
              "      <td>right</td>\n",
              "      <td>No agreement</td>\n",
              "      <td>No agreement</td>\n",
              "      <td>['flood']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The tragedy of America’s 18 years in Afghanist...</td>\n",
              "      <td>http://feedproxy.google.com/~r/breitbart/~3/ER...</td>\n",
              "      <td>breitbart</td>\n",
              "      <td>international-politics-and-world-news</td>\n",
              "      <td>right</td>\n",
              "      <td>Biased</td>\n",
              "      <td>Somewhat factual but also opinionated</td>\n",
              "      <td>['tragedy', 'stubborn']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The justices threw out a challenge from gun ri...</td>\n",
              "      <td>https://www.huffpost.com/entry/supreme-court-g...</td>\n",
              "      <td>msnbc</td>\n",
              "      <td>gun-control</td>\n",
              "      <td>left</td>\n",
              "      <td>Non-biased</td>\n",
              "      <td>Entirely factual</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d02bc569-9c1e-435e-9179-c7e0e8f69b9f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d02bc569-9c1e-435e-9179-c7e0e8f69b9f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d02bc569-9c1e-435e-9179-c7e0e8f69b9f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "PATH_sg1 = \"data/final_labels_SG1.xlsx\"\n",
        "PATH_sg2 = \"data/final_labels_SG2.xlsx\"\n",
        "df_sg1 = pd.read_excel(PATH_sg1)\n",
        "df_sg2 = pd.read_excel(PATH_sg2)\n",
        "df_sg1.rename(columns={'text': 'sentence', 'label_bias': 'Label_bias'}, inplace=True)\n",
        "df_sg2.rename(columns={'text': 'sentence', 'label_bias': 'Label_bias'}, inplace=True)\n",
        "df_sg1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8v7O9nyvYX8P"
      },
      "outputs": [],
      "source": [
        "# binarize classification problem\n",
        "df_sg1 = df_sg1[df_sg1['Label_bias']!='No agreement']\n",
        "df_sg1 = df_sg1[df_sg1['Label_bias'].isna()==False]\n",
        "df_sg1.replace(to_replace='Biased', value=1, inplace=True)\n",
        "df_sg1.replace(to_replace='Non-biased', value=0, inplace=True)\n",
        "\n",
        "df_sg2 = df_sg2[df_sg2['Label_bias']!='No agreement']\n",
        "df_sg2.replace(to_replace='Biased', value=1, inplace=True)\n",
        "df_sg2.replace(to_replace='Non-biased', value=0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pRzJDtTif2iH"
      },
      "outputs": [],
      "source": [
        "# Stratified k-Fold instance\n",
        "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzQYIT1wCG-m"
      },
      "source": [
        "The rest of the preprocessing needs to be performed inside the folds as a) encoder layers shouldn't be allowed to see whole data to construct the lookups and b) indexing with skfold is not possible when data is in tensorflow format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hXTxU-BNYX8P"
      },
      "outputs": [],
      "source": [
        "# helper functions called in skfold loop\n",
        "\n",
        "def pd_to_tf(df):\n",
        "    \"\"\"convert a pandas dataframe into a tensorflow dataset\"\"\"\n",
        "    target = df.pop('Label_bias')\n",
        "    sentence = df.pop('sentence')\n",
        "    return tf.data.Dataset.from_tensor_slices((sentence.values, target.values))\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()\n",
        "\n",
        "def tokenize(df):\n",
        "    \"\"\"convert a pandas dataframe into a tensorflow dataset and run hugging face's tokenizer on data\"\"\"\n",
        "    target = df.pop('Label_bias')\n",
        "    sentence = df.pop('sentence')\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "                        sentence.tolist(),                      \n",
        "                        add_special_tokens = True, # add [CLS], [SEP]\n",
        "                        truncation = True, # cut off at max length of the text that can go to BERT\n",
        "                        padding = True, # add [PAD] tokens\n",
        "                        return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "              )\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (dict(train_encodings), \n",
        "         target.tolist()))\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUMxPAGdYX8W"
      },
      "source": [
        "## Attention-based models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QIIWDGdwTZap"
      },
      "outputs": [],
      "source": [
        "def run_model_5fold(df_train, model_name, freeze_encoder=True, pretrained=False, plot=False):\n",
        "  \"\"\"\"freeze flags whether encoder layer should be frozen to not destroy transfer learning. Only set to false when enough data is provided\"\"\"\n",
        "\n",
        "  # these variables will be needed for skfold to select indices\n",
        "  Y = df_train['Label_bias']\n",
        "  X = df_train['sentence']\n",
        "\n",
        "  # hyperparams\n",
        "  BUFFER_SIZE = 10000\n",
        "  BATCH_SIZE = 32\n",
        "  k = 1\n",
        "\n",
        "  val_loss = []\n",
        "  val_acc = []\n",
        "  val_prec = []\n",
        "  val_rec = []\n",
        "  val_f1 = []\n",
        "  val_f1_micro = []\n",
        "  val_f1_wmacro = []\n",
        "\n",
        "  for train_index, val_index in skfold.split(X,Y):\n",
        "    print('### Start fold {}'.format(k))\n",
        "    \n",
        "    # split into train and validation set\n",
        "    train_dataset = df_train.iloc[train_index]\n",
        "    val_dataset = df_train.iloc[val_index]\n",
        "\n",
        "    # prepare data for transformer\n",
        "    train_dataset = tokenize(train_dataset)\n",
        "    val_dataset = tokenize(val_dataset)\n",
        "\n",
        "    # mini-batch it\n",
        "    train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # create new model\n",
        "    if model_name == 'bert':\n",
        "      model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "    if model_name == 'distilbert':\n",
        "      model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "    elif model_name == 'roberta':\n",
        "      model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "    elif model_name == 'electra':\n",
        "      model = TFElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator')\n",
        "    elif model_name == 'xlnet':\n",
        "      model = TFXLNetForSequenceClassification.from_pretrained('xlnet-base-cased')\n",
        "    elif model_name == 'convbert':\n",
        "      model = TFConvBertForSequenceClassification.from_pretrained(\"YituTech/conv-bert-base\")\n",
        "    elif model_name == 'deberta':\n",
        "      model = TFDebertaForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if freeze_encoder == True:\n",
        "      for w in model.get_layer(index=0).weights:\n",
        "        w._trainable = False\n",
        "\n",
        "    # compile it\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5) \n",
        "    model.compile(optimizer=optimizer, loss=model.hf_compute_loss) \n",
        "\n",
        "    # transfer learning\n",
        "    if pretrained == True:\n",
        "      model.get_layer(index=0).set_weights(trained_model_layer) # load bias-specific weights\n",
        "      #model.load_weights('./checkpoints/')\n",
        "    \n",
        "    # after 2 epochs without improvement, stop training\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "    # fit it\n",
        "    history = model.fit(train_dataset, epochs=10, validation_data = val_dataset, callbacks=[callback])\n",
        "    \n",
        "    # plot history\n",
        "    if plot:\n",
        "      plot_graphs(history,'loss')\n",
        "\n",
        "    # evaluate\n",
        "    loss = model.evaluate(val_dataset)\n",
        "    \n",
        "    if model_name == 'xlnet':\n",
        "      yhats = []\n",
        "      for row in df_train.iloc[val_index]['sentence']:\n",
        "        input = tokenizer(row, return_tensors=\"tf\")\n",
        "        output = model(input)\n",
        "        logits = output.logits.numpy()[0]\n",
        "        candidates = logits.tolist()\n",
        "        decision = candidates.index(max(candidates))\n",
        "        yhats.append(decision)\n",
        "    else:\n",
        "      logits = model.predict(val_dataset)  \n",
        "      yhats = []\n",
        "      for i in logits[0]:\n",
        "        # assign class label according to highest logit\n",
        "        candidates = i.tolist()\n",
        "        decision = candidates.index(max(candidates))\n",
        "        yhats.append(decision)\n",
        "    \n",
        "    y = []\n",
        "    for text, label in val_dataset.unbatch():   \n",
        "      y.append(label.numpy())\n",
        "    \n",
        "    val_loss.append(loss)\n",
        "    val_acc.append(accuracy_score(y, yhats))\n",
        "    val_prec.append(precision_score(y, yhats))\n",
        "    val_rec.append(recall_score(y, yhats))\n",
        "    val_f1.append(f1_score(y, yhats))\n",
        "    val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
        "    val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    k += 1\n",
        "\n",
        "  return val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCWs4EWjy4Kp"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "qUKSEvD4i_g4",
        "outputId": "61f5df8e-01e7-4cf0-8bff-b63a0f2fc556"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b6e3912d15c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "time.sleep(200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV-PUSinUJnF"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "\n",
        "\n",
        "df_train = df_sg1 \n",
        "model_name='bert' \n",
        "freeze_encoder=False\n",
        "pretrained=False\n",
        "\n",
        "\n",
        "  # these variables will be needed for skfold to select indices\n",
        "Y = df_train['Label_bias']\n",
        "X = df_train['sentence']\n",
        "\n",
        "# hyperparams\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 32\n",
        "k = 1\n",
        "\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "val_prec = []\n",
        "val_rec = []\n",
        "val_f1 = []\n",
        "val_f1_micro = []\n",
        "val_f1_wmacro = []\n",
        "\n",
        "for train_index, val_index in skfold.split(X,Y):\n",
        "  print('### Start fold {}'.format(k))\n",
        "  \n",
        "  # split into train and validation set\n",
        "  train_dataset = df_train.iloc[train_index]\n",
        "  val_dataset = df_train.iloc[val_index]\n",
        "  \n",
        "  # prepare data for transformer\n",
        "  train_dataset = tokenize(train_dataset)\n",
        "  val_dataset = tokenize(val_dataset)\n",
        "\n",
        "  # mini-batch it\n",
        "  train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "  # create new model\n",
        "\n",
        "  model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "  if freeze_encoder == True:\n",
        "    for w in model.get_layer(index=0).weights:\n",
        "      w._trainable = False\n",
        "\n",
        "  # compile it\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
        "  model.compile(optimizer=optimizer, loss=model.hf_compute_loss) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # print(\"Val_dataset: \", val_dataset)\n",
        "  # for a in val_dataset:\n",
        "  #   for b in a:\n",
        "  #     print(\"a: \", b)\n",
        "  #     print(\"NweLLine\")\n",
        "  #   print(\"GOING\")\n",
        "  # print(\"AND\")\n",
        "  # print(\"train_dataset: \", train_dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # transfer learning\n",
        "  if pretrained == True:\n",
        "    model.get_layer(index=0).set_weights(trained_model_layer) # load bias-specific weights\n",
        "    #model.load_weights('./checkpoints/')\n",
        "  \n",
        "  # after 2 epochs without improvement, stop training\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "  # # fit it\n",
        "  history = model.fit(train_dataset, epochs=10, validation_data = val_dataset, callbacks=[callback])\n",
        "  \n",
        "  # # plot history\n",
        "  # if plot:\n",
        "  #   plot_graphs(history,'loss')\n",
        "\n",
        "  # evaluate\n",
        "  # loss = model.evaluate(val_dataset)\n",
        "  \n",
        "  if model_name == 'xlnet':\n",
        "    yhats = []\n",
        "    for row in df_train.iloc[val_index]['sentence']:\n",
        "      input = tokenizer(row, return_tensors=\"tf\")\n",
        "      output = model(input)\n",
        "      logits = output.logits.numpy()[0]\n",
        "      candidates = logits.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  else:\n",
        "    logits = model.predict(val_dataset)  \n",
        "    yhats = []\n",
        "    for i in logits[0]:\n",
        "      # assign class label according to highest logit\n",
        "      candidates = i.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  \n",
        "  y = []\n",
        "  for text, label in val_dataset.unbatch():   \n",
        "    y.append(label.numpy())\n",
        "  \n",
        "  # val_loss.append(loss)\n",
        "  val_acc.append(accuracy_score(y, yhats))\n",
        "  val_prec.append(precision_score(y, yhats))\n",
        "  val_rec.append(recall_score(y, yhats))\n",
        "  val_f1.append(f1_score(y, yhats))\n",
        "  val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
        "  val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  k += 1\n",
        "\n",
        "\n",
        "# val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='bert', \n",
        "                                                                                            # freeze_encoder=False, pretrained=False)\n",
        "# inspect metrics\n",
        "# loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "# print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4OJmwP2ziMa"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI34Fj7zzAIh"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for BERT on SG2')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ],
      "metadata": {
        "id": "M0oaHNWUdJ-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for BERT on SG1')\n",
        "print('10-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('10-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('10-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('10-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('10-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('10-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('10-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "metadata": {
        "id": "5235twaIdMai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9t6QsLC5Oou"
      },
      "source": [
        "### BERT + distant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tufL8EEl5NvY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "71da0de369e24868941591006463b496",
            "48cdbd071c1745f88e8d40654cbc241e",
            "df8b2230be3d4a9b81b927888316a1e3",
            "83051d827a4f4148bfe0bef0336f68bf",
            "874e139541bc4000a14e95bd2289bcc1",
            "2765f23d25de47ff84b7d0bca65f4117",
            "cc3a2e39dae2484d82d93b3c15b2dbe9",
            "0fcf02b91e0a49f38d525a7480345a9c",
            "69a7030824504d018c33ebfd119d4cfa",
            "5eac21a01d944afaaa767f8bd101820e",
            "a80d96f1dbb64d6586e86c23e15f9c8b",
            "552c8b1ae3244b68ae9a42230a2676c8",
            "ae23131f15cf46fd87def09b9b6fdee8",
            "a583f58c8cab4dd1843d9d651c0b9b6b",
            "dfabb07514a145a49475b286591eca1c",
            "7bb398f92cf94d48bbd6f54f20a978ff",
            "dfc73d1c03124c6ca4bc43061b0c39c9",
            "93d75ffa6e6a4fb7b199348cf3256b16",
            "805f01ac63f84ad8a997b523285bfb48",
            "7eda82fe227540e3a99b112ceb577c72",
            "2458681341e54de0bc389a49592396a3",
            "97e5c5ca6a2d4860984be0ddc52658f5"
          ]
        },
        "outputId": "0d7c675a-7b14-4c4e-c6e3-3cd7805a73cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71da0de369e24868941591006463b496"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "552c8b1ae3244b68ae9a42230a2676c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# load model layer weights from pretraining on distant dataset \n",
        "# compile model\n",
        "#transfer_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "transfer_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "transfer_model.compile(optimizer=optimizer, loss=transfer_model.compute_loss) \n",
        "\n",
        "transfer_model.load_weights('./checkpoints/roberta_final_checkpoint_news_headlines_USA')\n",
        "trained_model_layer = transfer_model.get_layer(index=0).get_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Qp6TOmLP42nZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e0fba4513fb74431b40169f93342390c",
            "ae72c48ced54482f87375f225cc13e20",
            "0acb7e17cb31493b99d056d20ca890e1",
            "625b31b4e9cd47059e64c0a33b22d0b0",
            "20dd5866e792419a97350ccc02e7be89",
            "4c389f74d4d44b89a3259b5f8aa8b9bb",
            "cd0a4611904b46dd81182a60a7ebe8e2",
            "cb84317e1c374e78a6638e3f26b2be4e",
            "1f6e37b8f5b04901b38ff476dbce9c6f",
            "66b1496875bf461592b5852190bd5cac",
            "44dd78ad150f40e78db981d4e33407ff",
            "7eb4c9ee024141b4b2152603e2af7d39",
            "c5cf4cd659ba4cf3bcfe1b9cca8dd474",
            "d963157df5424a208451c68245c7d53b",
            "378fc88656d74c35979700a0ccaa0b90",
            "5058a193680c45689eba2a50579de31e",
            "e998ea87949e4ca199427d290a5559a6",
            "d19f75b256124e859a09b707d0193cd7",
            "05f9f62325564cf28adba723b914d57f",
            "0e7f594899504498b7d71f7283bc46c9",
            "cb50d6a541674953a94eb6d48cbdb4dd",
            "eb7e1cb2357a4de6a330e555c1cf2afc"
          ]
        },
        "outputId": "9d34d42a-4ee0-4618-eec1-9cee3768c31e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0fba4513fb74431b40169f93342390c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7eb4c9ee024141b4b2152603e2af7d39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Start fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.embeddings.weight\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.embeddings.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.embeddings.token_type_embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.embeddings.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.embeddings.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.0.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.1.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.2.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.3.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.4.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.5.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.6.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.7.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.8.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.9.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).roberta.encoder.layer.11.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.embeddings.weight\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.embeddings.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.embeddings.token_type_embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.embeddings.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.embeddings.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.0.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.1.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.2.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.3.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.4.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.5.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.6.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.7.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.8.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.9.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).roberta.encoder.layer.11.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.embeddings.weight\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.embeddings.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.embeddings.token_type_embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.embeddings.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.embeddings.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.intermediate.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.intermediate.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.bert_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.bert_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.bert_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.bert_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.0.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.1.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.2.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.3.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.4.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.5.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.6.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.7.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.8.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.9.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.10.attention.dense_output.LayerNorm.beta\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.self_attention.query.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.self_attention.query.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.self_attention.key.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.self_attention.key.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.self_attention.value.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.self_attention.value.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.dense_output.dense.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.dense_output.dense.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.dense_output.LayerNorm.gamma\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).roberta.encoder.layer.11.attention.dense_output.LayerNorm.beta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 56s 839ms/step - loss: 0.6174 - val_loss: 0.5240\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 33s 756ms/step - loss: 0.4783 - val_loss: 0.4354\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 33s 754ms/step - loss: 0.3294 - val_loss: 0.4520\n",
            "5/5 [==============================] - 1s 191ms/step - loss: 0.4354\n",
            "5/5 [==============================] - 4s 185ms/step\n",
            "### Start fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 52s 834ms/step - loss: 0.6202 - val_loss: 0.7282\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 33s 760ms/step - loss: 0.4737 - val_loss: 0.6147\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 33s 755ms/step - loss: 0.3012 - val_loss: 0.5831\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 34s 761ms/step - loss: 0.1261 - val_loss: 0.8810\n",
            "5/5 [==============================] - 1s 197ms/step - loss: 0.5831\n",
            "5/5 [==============================] - 4s 193ms/step\n",
            "### Start fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 53s 871ms/step - loss: 0.6172 - val_loss: 0.5293\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 34s 772ms/step - loss: 0.4623 - val_loss: 0.4697\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 34s 769ms/step - loss: 0.2923 - val_loss: 0.5068\n",
            "5/5 [==============================] - 1s 261ms/step - loss: 0.4697\n",
            "5/5 [==============================] - 4s 250ms/step\n",
            "### Start fold 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 51s 834ms/step - loss: 0.6090 - val_loss: 0.5987\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 33s 760ms/step - loss: 0.4552 - val_loss: 0.5765\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 33s 751ms/step - loss: 0.2916 - val_loss: 0.6103\n",
            "5/5 [==============================] - 1s 202ms/step - loss: 0.5765\n",
            "5/5 [==============================] - 4s 192ms/step\n",
            "### Start fold 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 53s 835ms/step - loss: 0.6173 - val_loss: 0.5704\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 33s 756ms/step - loss: 0.4175 - val_loss: 0.5273\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 33s 753ms/step - loss: 0.2452 - val_loss: 0.6034\n",
            "5/5 [==============================] - 1s 189ms/step - loss: 0.5273\n",
            "5/5 [==============================] - 4s 183ms/step\n",
            "### Start fold 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 53s 837ms/step - loss: 0.6346 - val_loss: 0.5973\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 33s 759ms/step - loss: 0.5031 - val_loss: 0.4905\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 33s 754ms/step - loss: 0.3679 - val_loss: 0.5660\n",
            "5/5 [==============================] - 1s 194ms/step - loss: 0.4905\n",
            "5/5 [==============================] - 4s 189ms/step\n",
            "### Start fold 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 53s 835ms/step - loss: 0.6316 - val_loss: 0.5924\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 34s 761ms/step - loss: 0.4626 - val_loss: 0.5561\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 33s 759ms/step - loss: 0.2932 - val_loss: 0.6335\n",
            "5/5 [==============================] - 1s 213ms/step - loss: 0.5561\n",
            "5/5 [==============================] - 4s 209ms/step\n",
            "### Start fold 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 53s 861ms/step - loss: 0.6352 - val_loss: 0.5886\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 33s 756ms/step - loss: 0.4674 - val_loss: 0.6171\n",
            "5/5 [==============================] - 1s 196ms/step - loss: 0.5886\n",
            "5/5 [==============================] - 4s 194ms/step\n",
            "### Start fold 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 52s 839ms/step - loss: 0.6466 - val_loss: 0.5316\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 34s 764ms/step - loss: 0.5344 - val_loss: 0.5475\n",
            "5/5 [==============================] - 1s 253ms/step - loss: 0.5316\n",
            "5/5 [==============================] - 4s 245ms/step\n",
            "### Start fold 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 54s 832ms/step - loss: 0.6302 - val_loss: 0.5775\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 34s 763ms/step - loss: 0.5385 - val_loss: 0.4670\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 33s 757ms/step - loss: 0.3488 - val_loss: 0.4380\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 33s 760ms/step - loss: 0.1871 - val_loss: 0.4836\n",
            "5/5 [==============================] - 1s 198ms/step - loss: 0.4380\n",
            "5/5 [==============================] - 4s 192ms/step\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# with distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JGq54KK_5u1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9c426b-0cf3-4ff3-a5b8-4bcc02314d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for BERT + distant on SG1\n",
            "10-Fold CV Loss: 0.5196687072515488\n",
            "10-Fold CV Accuracy: 0.7632551319648093\n",
            "10-Fold CV Precision: 0.7828247128144844\n",
            "10-Fold CV Recall: 0.7181801801801801\n",
            "10-Fold CV F1 Score: 0.7429367998666617\n",
            "10-Fold CV Micro F1 Score: 0.7632551319648093\n",
            "10-Fold CV Weighted Macro F1 Score: 0.7608342066327985\n"
          ]
        }
      ],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for BERT + distant on SG1')\n",
        "print('10-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('10-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('10-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('10-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('10-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('10-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('10-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss4XWht750Ff"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# with distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='bert', \n",
        "                                                                                            freeze_encoder=False, pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmIN7OV155p3"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for BERT + distant on SG2')\n",
        "print('10-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('10-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('10-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('10-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('10-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('10-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('10-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCW0oaxBzonB"
      },
      "source": [
        "### DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqw59Djszq2X"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='distilbert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j4gNOr9z9Tk"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for DistilBERT on SG1')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHqfzE-jz4lm"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='distilbert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kt0gOdy0ALD"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for DistilBERT on SG2')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aYaKZL8o6_LA"
      },
      "outputs": [],
      "source": [
        "#@title TFALBERT\n",
        "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"vumichien/albert-base-v2-imdb\")\n",
        "\n",
        "\n",
        "\n",
        "df_train = df_sg1 \n",
        "model_name='albert' \n",
        "freeze_encoder=False\n",
        "pretrained=False\n",
        "\n",
        "\n",
        "  # these variables will be needed for skfold to select indices\n",
        "Y = df_train['Label_bias']\n",
        "X = df_train['sentence']\n",
        "\n",
        "# hyperparams\n",
        "BUFFER_SIZE = 250\n",
        "BATCH_SIZE = 32\n",
        "k = 1\n",
        "\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "val_prec = []\n",
        "val_rec = []\n",
        "val_f1 = []\n",
        "val_f1_micro = []\n",
        "val_f1_wmacro = []\n",
        "\n",
        "for train_index, val_index in skfold.split(X,Y):\n",
        "  print('### Start fold {}'.format(k))\n",
        "  \n",
        "  # split into train and validation set\n",
        "  train_dataset = df_train.iloc[train_index]\n",
        "  val_dataset = df_train.iloc[val_index]\n",
        "  \n",
        "  # prepare data for transformer\n",
        "  train_dataset = tokenize(train_dataset)\n",
        "  val_dataset = tokenize(val_dataset)\n",
        "\n",
        "  # mini-batch it\n",
        "  train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # create new model\n",
        "\n",
        "  model = TFAlbertForSequenceClassification.from_pretrained(\"vumichien/albert-base-v2-imdb\")\n",
        "\n",
        "  # compile it\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
        "  model.compile(optimizer=optimizer, loss=model.hf_compute_loss) \n",
        "\n",
        "  \n",
        "  # after 2 epochs without improvement, stop training\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "  # # fit it\n",
        "  history = model.fit(train_dataset, epochs=10, validation_data = val_dataset, callbacks=[callback])\n",
        "  \n",
        "  # # plot history\n",
        "  # if plot:\n",
        "  #   plot_graphs(history,'loss')\n",
        "\n",
        "  # evaluate\n",
        "  # loss = model.evaluate(val_dataset)\n",
        "  \n",
        "  if model_name == 'xlnet':\n",
        "    yhats = []\n",
        "    for row in df_train.iloc[val_index]['sentence']:\n",
        "      input = tokenizer(row, return_tensors=\"tf\")\n",
        "      output = model(input)\n",
        "      logits = output.logits.numpy()[0]\n",
        "      candidates = logits.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  else:\n",
        "    logits = model.predict(val_dataset)  \n",
        "    yhats = []\n",
        "    for i in logits[0]:\n",
        "      # assign class label according to highest logit\n",
        "      candidates = i.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  \n",
        "  y = []\n",
        "  for text, label in val_dataset.unbatch():   \n",
        "    y.append(label.numpy())\n",
        "  \n",
        "  # val_loss.append(loss)\n",
        "  val_acc.append(accuracy_score(y, yhats))\n",
        "  val_prec.append(precision_score(y, yhats))\n",
        "  val_rec.append(recall_score(y, yhats))\n",
        "  val_f1.append(f1_score(y, yhats))\n",
        "  val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
        "  val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  k += 1\n",
        "\n",
        "\n",
        "# val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='bert', \n",
        "                                                                                            # freeze_encoder=False, pretrained=False)\n",
        "# inspect metrics\n",
        "# loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "# print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_FFtvdUaXMs"
      },
      "outputs": [],
      "source": [
        "  tokenizer = AlbertTokenizer.from_pretrained(\"vumichien/albert-base-v2-imdb\")\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='alberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AT6p3bLKet-D"
      },
      "outputs": [],
      "source": [
        "#@title CTRL\n",
        "from transformers import CTRLTokenizer, TFCTRLForSequenceClassification\n",
        "\n",
        "\n",
        "tokenizer = CTRLTokenizer.from_pretrained(\"ctrl\")\n",
        "\n",
        "\n",
        "df_train = df_sg1 \n",
        "model_name='albert' \n",
        "freeze_encoder=False\n",
        "pretrained=False\n",
        "\n",
        "\n",
        "  # these variables will be needed for skfold to select indices\n",
        "Y = df_train['Label_bias']\n",
        "X = df_train['sentence']\n",
        "\n",
        "# hyperparams\n",
        "BUFFER_SIZE = 250\n",
        "BATCH_SIZE = 32\n",
        "k = 1\n",
        "\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "val_prec = []\n",
        "val_rec = []\n",
        "val_f1 = []\n",
        "val_f1_micro = []\n",
        "val_f1_wmacro = []\n",
        "\n",
        "for train_index, val_index in skfold.split(X,Y):\n",
        "  print('### Start fold {}'.format(k))\n",
        "  \n",
        "  # split into train and validation set\n",
        "  train_dataset = df_train.iloc[train_index]\n",
        "  val_dataset = df_train.iloc[val_index]\n",
        "  \n",
        "  if tokenizer.pad_token is None:\n",
        "      tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "  # prepare data for transformer\n",
        "  train_dataset = tokenize(train_dataset)\n",
        "  \n",
        "  val_dataset = tokenize(val_dataset)\n",
        "\n",
        "\n",
        "  # mini-batch it\n",
        "  train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # create new model\n",
        "\n",
        "  model = TFCTRLForSequenceClassification.from_pretrained(\"ctrl\")\n",
        "\n",
        "\n",
        "\n",
        "  # compile it\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
        "  model.compile(optimizer=optimizer, loss=model.hf_compute_loss) \n",
        "\n",
        "  \n",
        "  # after 2 epochs without improvement, stop training\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "  # # fit it\n",
        "  history = model.fit(train_dataset, epochs=10, validation_data = val_dataset, callbacks=[callback])\n",
        "  \n",
        "  # # plot history\n",
        "  # if plot:\n",
        "  #   plot_graphs(history,'loss')\n",
        "\n",
        "  # evaluate\n",
        "  # loss = model.evaluate(val_dataset)\n",
        "  \n",
        "  if model_name == 'xlnet':\n",
        "    yhats = []\n",
        "    for row in df_train.iloc[val_index]['sentence']:\n",
        "      input = tokenizer(row, return_tensors=\"tf\")\n",
        "      output = model(input)\n",
        "      logits = output.logits.numpy()[0]\n",
        "      candidates = logits.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  else:\n",
        "    logits = model.predict(val_dataset)  \n",
        "    yhats = []\n",
        "    for i in logits[0]:\n",
        "      # assign class label according to highest logit\n",
        "      candidates = i.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  \n",
        "  y = []\n",
        "  for text, label in val_dataset.unbatch():   \n",
        "    y.append(label.numpy())\n",
        "  \n",
        "  # val_loss.append(loss)\n",
        "  val_acc.append(accuracy_score(y, yhats))\n",
        "  val_prec.append(precision_score(y, yhats))\n",
        "  val_rec.append(recall_score(y, yhats))\n",
        "  val_f1.append(f1_score(y, yhats))\n",
        "  val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
        "  val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  k += 1\n",
        "\n",
        "\n",
        "# val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='bert', \n",
        "                                                                                            # freeze_encoder=False, pretrained=False)\n",
        "# inspect metrics\n",
        "# loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "# print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k33rZcAl-opx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4a_5HQk-pXR"
      },
      "outputs": [],
      "source": [
        "#@title Deberta\n",
        "from transformers import DebertaTokenizer, TFDebertaForSequenceClassification\n",
        "\n",
        "\n",
        "tokenizer = DebertaTokenizer.from_pretrained(\"kamalkraj/deberta-base\")\n",
        "\n",
        "\n",
        "df_train = df_sg1 \n",
        "model_name='albert' \n",
        "freeze_encoder=False\n",
        "pretrained=False\n",
        "\n",
        "\n",
        "  # these variables will be needed for skfold to select indices\n",
        "Y = df_train['Label_bias']\n",
        "X = df_train['sentence']\n",
        "\n",
        "# hyperparams\n",
        "BUFFER_SIZE = 250\n",
        "BATCH_SIZE = 32\n",
        "k = 1\n",
        "\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "val_prec = []\n",
        "val_rec = []\n",
        "val_f1 = []\n",
        "val_f1_micro = []\n",
        "val_f1_wmacro = []\n",
        "\n",
        "for train_index, val_index in skfold.split(X,Y):\n",
        "  print('### Start fold {}'.format(k))\n",
        "  \n",
        "  # split into train and validation set\n",
        "  train_dataset = df_train.iloc[train_index]\n",
        "  val_dataset = df_train.iloc[val_index]\n",
        "  \n",
        "  if tokenizer.pad_token is None:\n",
        "      tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "  # prepare data for transformer\n",
        "  train_dataset = tokenize(train_dataset)\n",
        "  \n",
        "  val_dataset = tokenize(val_dataset)\n",
        "\n",
        "\n",
        "  # mini-batch it\n",
        "  train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # create new model\n",
        "\n",
        "  model = TFDebertaForSequenceClassification.from_pretrained(\"kamalkraj/deberta-base\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # compile it\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
        "  model.compile(optimizer=optimizer, loss=model.hf_compute_loss) \n",
        "\n",
        "  \n",
        "  # after 2 epochs without improvement, stop training\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "  # # fit it\n",
        "  history = model.fit(train_dataset, epochs=10, validation_data = val_dataset, callbacks=[callback])\n",
        "  \n",
        "  # # plot history\n",
        "  # if plot:\n",
        "  #   plot_graphs(history,'loss')\n",
        "\n",
        "  # evaluate\n",
        "  # loss = model.evaluate(val_dataset)\n",
        "  \n",
        "  if model_name == 'xlnet':\n",
        "    yhats = []\n",
        "    for row in df_train.iloc[val_index]['sentence']:\n",
        "      input = tokenizer(row, return_tensors=\"tf\")\n",
        "      output = model(input)\n",
        "      logits = output.logits.numpy()[0]\n",
        "      candidates = logits.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  else:\n",
        "    logits = model.predict(val_dataset)  \n",
        "    yhats = []\n",
        "    for i in logits[0]:\n",
        "      # assign class label according to highest logit\n",
        "      candidates = i.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  \n",
        "  y = []\n",
        "  for text, label in val_dataset.unbatch():   \n",
        "    y.append(label.numpy())\n",
        "  \n",
        "  # val_loss.append(loss)\n",
        "  val_acc.append(accuracy_score(y, yhats))\n",
        "  val_prec.append(precision_score(y, yhats))\n",
        "  val_rec.append(recall_score(y, yhats))\n",
        "  val_f1.append(f1_score(y, yhats))\n",
        "  val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
        "  val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  k += 1\n",
        "\n",
        "\n",
        "# val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='bert', \n",
        "                                                                                            # freeze_encoder=False, pretrained=False)\n",
        "# inspect metrics\n",
        "# loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "# print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A4beOkOgCRcR"
      },
      "outputs": [],
      "source": [
        "#@title DebertaV2\n",
        "from transformers import DebertaV2Tokenizer, TFDebertaV2ForSequenceClassification\n",
        "\n",
        "\n",
        "tokenizer = DebertaV2Tokenizer.from_pretrained(\"kamalkraj/deberta-v2-xlarge\")\n",
        "\n",
        "\n",
        "df_train = df_sg1 \n",
        "model_name='albert' \n",
        "freeze_encoder=False\n",
        "pretrained=False\n",
        "\n",
        "\n",
        "  # these variables will be needed for skfold to select indices\n",
        "Y = df_train['Label_bias']\n",
        "X = df_train['sentence']\n",
        "\n",
        "# hyperparams\n",
        "BUFFER_SIZE = 250\n",
        "BATCH_SIZE = 32\n",
        "k = 1\n",
        "\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "val_prec = []\n",
        "val_rec = []\n",
        "val_f1 = []\n",
        "val_f1_micro = []\n",
        "val_f1_wmacro = []\n",
        "\n",
        "for train_index, val_index in skfold.split(X,Y):\n",
        "  print('### Start fold {}'.format(k))\n",
        "  \n",
        "  # split into train and validation set\n",
        "  train_dataset = df_train.iloc[train_index]\n",
        "  val_dataset = df_train.iloc[val_index]\n",
        "  \n",
        "  if tokenizer.pad_token is None:\n",
        "      tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "  # prepare data for transformer\n",
        "  train_dataset = tokenize(train_dataset)\n",
        "  \n",
        "  val_dataset = tokenize(val_dataset)\n",
        "\n",
        "\n",
        "  # mini-batch it\n",
        "  train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # create new model\n",
        "\n",
        "  model = TFDebertaV2ForSequenceClassification.from_pretrained(\"kamalkraj/deberta-v2-xlarge\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # compile it\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
        "  model.compile(optimizer=optimizer, loss=model.hf_compute_loss) \n",
        "\n",
        "  \n",
        "  # after 2 epochs without improvement, stop training\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "  # # fit it\n",
        "  history = model.fit(train_dataset, epochs=10, validation_data = val_dataset, callbacks=[callback])\n",
        "  \n",
        "  # # plot history\n",
        "  # if plot:\n",
        "  #   plot_graphs(history,'loss')\n",
        "\n",
        "  # evaluate\n",
        "  # loss = model.evaluate(val_dataset)\n",
        "  \n",
        "  if model_name == 'xlnet':\n",
        "    yhats = []\n",
        "    for row in df_train.iloc[val_index]['sentence']:\n",
        "      input = tokenizer(row, return_tensors=\"tf\")\n",
        "      output = model(input)\n",
        "      logits = output.logits.numpy()[0]\n",
        "      candidates = logits.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  else:\n",
        "    logits = model.predict(val_dataset)  \n",
        "    yhats = []\n",
        "    for i in logits[0]:\n",
        "      # assign class label according to highest logit\n",
        "      candidates = i.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  \n",
        "  y = []\n",
        "  for text, label in val_dataset.unbatch():   \n",
        "    y.append(label.numpy())\n",
        "  \n",
        "  # val_loss.append(loss)\n",
        "  val_acc.append(accuracy_score(y, yhats))\n",
        "  val_prec.append(precision_score(y, yhats))\n",
        "  val_rec.append(recall_score(y, yhats))\n",
        "  val_f1.append(f1_score(y, yhats))\n",
        "  val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
        "  val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  k += 1\n",
        "\n",
        "\n",
        "# val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='bert', \n",
        "                                                                                            # freeze_encoder=False, pretrained=False)\n",
        "# inspect metrics\n",
        "# loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "# print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_rYDkwakSA4"
      },
      "outputs": [],
      "source": [
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aqQxI-Lwj0on"
      },
      "outputs": [],
      "source": [
        "#@title FlauBERT\n",
        "from transformers import FlaubertTokenizer, TFFlaubertForSequenceClassification\n",
        "\n",
        "\n",
        "tokenizer = FlaubertTokenizer.from_pretrained(\"flaubert/flaubert_base_cased\")\n",
        "\n",
        "\n",
        "df_train = df_sg1 \n",
        "model_name='albert' \n",
        "freeze_encoder=False\n",
        "pretrained=False\n",
        "\n",
        "\n",
        "  # these variables will be needed for skfold to select indices\n",
        "Y = df_train['Label_bias']\n",
        "X = df_train['sentence']\n",
        "\n",
        "# hyperparams\n",
        "BUFFER_SIZE = 250\n",
        "BATCH_SIZE = 32\n",
        "k = 1\n",
        "\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "val_prec = []\n",
        "val_rec = []\n",
        "val_f1 = []\n",
        "val_f1_micro = []\n",
        "val_f1_wmacro = []\n",
        "\n",
        "for train_index, val_index in skfold.split(X,Y):\n",
        "  print('### Start fold {}'.format(k))\n",
        "  \n",
        "  # split into train and validation set\n",
        "  train_dataset = df_train.iloc[train_index]\n",
        "  val_dataset = df_train.iloc[val_index]\n",
        "  \n",
        "  if tokenizer.pad_token is None:\n",
        "      tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "  # prepare data for transformer\n",
        "  train_dataset = tokenize(train_dataset)\n",
        "  \n",
        "  val_dataset = tokenize(val_dataset)\n",
        "\n",
        "\n",
        "  # mini-batch it\n",
        "  train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # create new model\n",
        "\n",
        "  model = TFFlaubertForSequenceClassification.from_pretrained(\"flaubert/flaubert_base_cased\", from_pt=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # compile it\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5) \n",
        "  model.compile(optimizer=optimizer, loss=model.hf_compute_loss) \n",
        "\n",
        "  \n",
        "  # after 2 epochs without improvement, stop training\n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "\n",
        "  # # fit it\n",
        "  history = model.fit(train_dataset, epochs=10, validation_data = val_dataset, callbacks=[callback])\n",
        "  \n",
        "  # # plot history\n",
        "  # if plot:\n",
        "  #   plot_graphs(history,'loss')\n",
        "\n",
        "  # evaluate\n",
        "  # loss = model.evaluate(val_dataset)\n",
        "  \n",
        "  if model_name == 'xlnet':\n",
        "    yhats = []\n",
        "    for row in df_train.iloc[val_index]['sentence']:\n",
        "      input = tokenizer(row, return_tensors=\"tf\")\n",
        "      output = model(input)\n",
        "      logits = output.logits.numpy()[0]\n",
        "      candidates = logits.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  else:\n",
        "    logits = model.predict(val_dataset)  \n",
        "    yhats = []\n",
        "    for i in logits[0]:\n",
        "      # assign class label according to highest logit\n",
        "      candidates = i.tolist()\n",
        "      decision = candidates.index(max(candidates))\n",
        "      yhats.append(decision)\n",
        "  \n",
        "  y = []\n",
        "  for text, label in val_dataset.unbatch():   \n",
        "    y.append(label.numpy())\n",
        "  \n",
        "  # val_loss.append(loss)\n",
        "  val_acc.append(accuracy_score(y, yhats))\n",
        "  val_prec.append(precision_score(y, yhats))\n",
        "  val_rec.append(recall_score(y, yhats))\n",
        "  val_f1.append(f1_score(y, yhats))\n",
        "  val_f1_micro.append(f1_score(y, yhats, average='micro'))\n",
        "  val_f1_wmacro.append(f1_score(y, yhats, average='weighted'))\n",
        "\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  k += 1\n",
        "\n",
        "\n",
        "# val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='bert', \n",
        "                                                                                            # freeze_encoder=False, pretrained=False)\n",
        "# inspect metrics\n",
        "# loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "# print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF5yK3bQgW71"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/markusschanta/advent-of-code-2022/blame/main/2022/04/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRTchi-m2hYh"
      },
      "source": [
        "### RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSUsyFiz2kTV"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmGQKU3h22jD"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for RoBERTa on SG1')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajU2IyFX2563"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJCJpFDy3AY5"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for RoBERTa on SG2')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOMwgdo9H2Ax"
      },
      "source": [
        "### RoBERTa + distant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xgmGx_olHxw0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "92a3b3bd20f44f55ada21c9d3209ac53",
            "adfee37b9ccf4ec3a97fc88736374d2a",
            "53fc9077d1374d13aa9cd01756bfae00",
            "1a8ea25d87644a16a6d2ed8f6cd58dd1",
            "30690007f3284ca69071ee45b514f8cb",
            "7331fab31a1f41068a3e691a89dd520c",
            "d3f53874b25b4b41830671c2b24e4a61",
            "4eec7b445ff84cefbf2cb9741ec8b84c",
            "dc0a8f60a9d045d1a22e57f0f0939f58",
            "206fdf9846774cc693f7788d289549b6",
            "494ff39b996a49d8abd08dd3613903a2",
            "fca27cf5a95c47f288b9449ee7d13608",
            "9382706bb86049ea93ca2d5140d6bddf",
            "61585c1d920f4a978ebd974a8060fb9a",
            "b7e3f119310642dc85cc53300d565da0",
            "9c53147a7124405c8d1dd96fc8181dda",
            "b51648f73ef14dcb90088e64f3d6279c",
            "13f26fc94b404cae8b4e0fb931bb07f8",
            "f20f193200264c22a80318ba78f48c43",
            "15154a5731464b7b81b36e0309dc62a2",
            "d2bb71996586492ab83ce3b0ace80774",
            "fc6e96513f3d4aadae85f25c0d8667c0"
          ]
        },
        "outputId": "41548fb8-9050-40fe-f45e-b4e7ddef6fbc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92a3b3bd20f44f55ada21c9d3209ac53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/657M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fca27cf5a95c47f288b9449ee7d13608"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# load model layer weights from pretraining on distant dataset \n",
        "# compile model\n",
        "transfer_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "transfer_model.compile(optimizer=optimizer, loss=transfer_model.compute_loss) \n",
        "\n",
        "transfer_model.load_weights('./checkpoints/roberta_final_checkpoint_news_headlines_USA')\n",
        "trained_model_layer = transfer_model.get_layer(index=0).get_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pevLAcJCH_qD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e15af236028c4126a3bc8600d5a68023",
            "7d782b207dd44476af552f37eac3863f",
            "8faebe4074eb43628ad6f0ecf509a9d1",
            "0efa3632210c4f839101e72f2d489974",
            "dcf5df4a1d0a4abf80860dde82ce6a91",
            "5eece09ce2e94668895b112e39593a57",
            "45463475851e413c840493c72bfa6b49",
            "2a29a30d58184098b7b56acfed7e6a0d",
            "41b6ff0a1e314d478a6d3525a6fd270c",
            "50298701d21449d3b332c70ec4d09a0a",
            "156b716f82674f94a59f9e638d978fef",
            "37e262a4dc3f414ebdf30dd8278846b7",
            "efe90acb88574093bf871b94310c3c02",
            "58dc0aabba4848c4bfa5ccf267b65c55",
            "7f76af93c0ac49fc8952076c07dbcf71",
            "43f32fda900346a284da7a00a38f620c",
            "7005abd674224c6bb330bfed7902366d",
            "236c0601bb4147619ae2ea3410eed35c",
            "71337aa752f749489058a82d74b775ac",
            "11bc23ab3e1a45109b291fd16f333d8a",
            "bd55d1aae93b49cea3178ec7837043a0",
            "b3bac04d6eb54f0e82f9ebb4512e6efa"
          ]
        },
        "outputId": "58715410-d1e8-4041-b03f-8dbec0d7fe13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e15af236028c4126a3bc8600d5a68023"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37e262a4dc3f414ebdf30dd8278846b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Start fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 57s 881ms/step - loss: 0.5575 - val_loss: 0.3318\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 36s 810ms/step - loss: 0.3888 - val_loss: 0.2891\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 35s 798ms/step - loss: 0.2808 - val_loss: 0.2486\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 35s 803ms/step - loss: 0.1635 - val_loss: 0.3077\n",
            "5/5 [==============================] - 1s 197ms/step - loss: 0.2486\n",
            "5/5 [==============================] - 4s 192ms/step\n",
            "### Start fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 54s 885ms/step - loss: 0.5399 - val_loss: 0.5140\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 35s 797ms/step - loss: 0.3576 - val_loss: 0.5656\n",
            "5/5 [==============================] - 1s 199ms/step - loss: 0.5140\n",
            "5/5 [==============================] - 4s 193ms/step\n",
            "### Start fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 55s 887ms/step - loss: 0.5460 - val_loss: 0.4222\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 36s 813ms/step - loss: 0.3840 - val_loss: 0.3652\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 35s 805ms/step - loss: 0.2640 - val_loss: 0.4129\n",
            "5/5 [==============================] - 1s 257ms/step - loss: 0.3652\n",
            "5/5 [==============================] - 4s 250ms/step\n",
            "### Start fold 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 54s 881ms/step - loss: 0.5219 - val_loss: 0.4491\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 35s 797ms/step - loss: 0.3468 - val_loss: 0.4591\n",
            "5/5 [==============================] - 1s 203ms/step - loss: 0.4491\n",
            "5/5 [==============================] - 4s 198ms/step\n",
            "### Start fold 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 56s 881ms/step - loss: 0.5513 - val_loss: 0.5026\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 35s 799ms/step - loss: 0.3925 - val_loss: 0.5256\n",
            "5/5 [==============================] - 1s 202ms/step - loss: 0.5026\n",
            "5/5 [==============================] - 4s 199ms/step\n",
            "### Start fold 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 54s 875ms/step - loss: 0.5490 - val_loss: 0.5266\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 35s 802ms/step - loss: 0.3652 - val_loss: 0.5096\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 35s 799ms/step - loss: 0.2321 - val_loss: 0.6031\n",
            "5/5 [==============================] - 1s 189ms/step - loss: 0.5096\n",
            "5/5 [==============================] - 4s 186ms/step\n",
            "### Start fold 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 55s 883ms/step - loss: 0.5254 - val_loss: 0.4949\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 35s 797ms/step - loss: 0.3531 - val_loss: 0.5077\n",
            "5/5 [==============================] - 1s 200ms/step - loss: 0.4949\n",
            "5/5 [==============================] - 4s 195ms/step\n",
            "### Start fold 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 55s 880ms/step - loss: 0.5424 - val_loss: 0.5376\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 35s 800ms/step - loss: 0.3630 - val_loss: 0.4580\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 35s 794ms/step - loss: 0.2627 - val_loss: 0.6294\n",
            "5/5 [==============================] - 1s 180ms/step - loss: 0.4580\n",
            "5/5 [==============================] - 4s 177ms/step\n",
            "### Start fold 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 55s 888ms/step - loss: 0.5341 - val_loss: 0.4226\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 35s 803ms/step - loss: 0.3639 - val_loss: 0.4255\n",
            "5/5 [==============================] - 1s 251ms/step - loss: 0.4226\n",
            "5/5 [==============================] - 4s 245ms/step\n",
            "### Start fold 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 54s 878ms/step - loss: 0.5331 - val_loss: 0.4574\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 35s 803ms/step - loss: 0.3784 - val_loss: 0.4124\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 35s 799ms/step - loss: 0.2548 - val_loss: 0.4595\n",
            "5/5 [==============================] - 1s 196ms/step - loss: 0.4124\n",
            "5/5 [==============================] - 4s 190ms/step\n"
          ]
        }
      ],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Vs_brFmCIDPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1284a55-7a94-498f-9ea6-4ccd7d322854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for RoBERTa + distant on SG1\n",
            "10-Fold CV Loss: 0.4377031669020653\n",
            "10-Fold CV Accuracy: 0.8117469627147047\n",
            "10-Fold CV Precision: 0.8320801665489419\n",
            "10-Fold CV Recall: 0.7679819819819821\n",
            "10-Fold CV F1 Score: 0.7960682866695172\n",
            "10-Fold CV Micro F1 Score: 0.8117469627147047\n",
            "10-Fold CV Weighted Macro F1 Score: 0.8106482285827301\n"
          ]
        }
      ],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for RoBERTa + distant on SG1')\n",
        "print('10-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('10-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('10-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('10-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('10-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('10-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('10-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ_FG4ysIAVi"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='roberta', \n",
        "                                                                                            freeze_encoder=False, pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqneQf2yIFmR"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for RoBERTa + distant on SG2')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVwXBK-03SbE"
      },
      "source": [
        "### ELECTRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW7tbu6f3RqZ"
      },
      "outputs": [],
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='electra', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZLodIFy3o7l"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for ELECTRA on SG1')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds_QU-K43jgD"
      },
      "outputs": [],
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='electra', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CvLJFmM3rgB"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for ELECTRA on SG2')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhsBtQ_S3tcC"
      },
      "source": [
        "### XLNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49GtDW9A3u79"
      },
      "outputs": [],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='xlnet', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L18ta1df3_8R"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for XLNET on SG1')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR8tCwlo37bb"
      },
      "outputs": [],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='xlnet', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgr6g6HG4FVX"
      },
      "outputs": [],
      "source": [
        "# inspect metrics\n",
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for XLNET on SG2')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YurPoAaF8v8l"
      },
      "outputs": [],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='xlnet', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI6iyAvR9BnW"
      },
      "source": [
        "### ConvBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7hle4U0UYb5j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597,
          "referenced_widgets": [
            "211ba498c09c4721b8c1eeb740f18ff4",
            "20cc25fa564747aa8adad5ec01a0e739",
            "1a51639257d9411fb3b3cbe739e24ea7",
            "e4f2939fe6f3442b88dc11577271f721",
            "beb98d84e97641f7bbc6201185df72d9",
            "080a6f5231d4452391702db3d4db9050",
            "805b6a4f75b947ea8f118572498f43ba",
            "2bf4a6c0bb8043d08d57cd7ad09c949d",
            "6319d5fbedb54d6d8bdebef0503f9774",
            "1962f05fd1e54c179c621104a2aef09a",
            "f097848fddd34c12a03e4b016a82934f",
            "fa575313d8054a98976fce778fa73753",
            "e4fdbdfd3870421caf5c8a7f6d317a98",
            "541d398b3add4b6a913fb26670a41c14",
            "ffcfbcf80bb943b899bbd50cca9393d0",
            "50a3b349f2a64429abef6209a4b17933",
            "afdbb109545744c0991ef2a223235f9e",
            "fcfe7c47a36d432ba553e22fc032c3c2",
            "beba76dd99724dd59b978834028d3c63",
            "192390afc4f04226a25eedc4c9249b1f",
            "25bc3257506d44dd884f3de4557032d8",
            "639f3277dd41436d8163a4420f40437c",
            "5866004c14a34ddea40e031e7fd4861e",
            "a09aebad8195440dac2bab5630730d9f",
            "6ad0f08f28914f6ab5cbd1096681ceb5",
            "c2ba956b2b634a7fa10103e8e0ceeb91",
            "98ecaeb6f0e44095b963fc1a0be01e4c",
            "fdc0085805a649f4822bac0fd4b3249d",
            "7f8e8183988342dd93cc0fdd61869419",
            "dde19e341c3f45b0806b6f1d0911c20c",
            "6beeacbdd3a6469091546c2d3b44e47d",
            "960a3ea43c5b40b4824cd1a1a9001d39",
            "e5fe9b1161914854b6700cbbdc6e9384"
          ]
        },
        "outputId": "76dc3e5c-a07b-4802-d9ab-b984be66d33f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/267k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "211ba498c09c4721b8c1eeb740f18ff4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/674 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa575313d8054a98976fce778fa73753"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Start fold 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/423M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5866004c14a34ddea40e031e7fd4861e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 76s 1s/step - loss: 0.6421 - val_loss: 0.5012\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a30eda1cf5f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# without distant signal pretraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='convbert', \n\u001b[0m\u001b[1;32m      5\u001b[0m                                                                                             freeze_encoder=False, pretrained=False)\n",
            "\u001b[0;32m<ipython-input-13-abbd614c47f1>\u001b[0m in \u001b[0;36mrun_model_5fold\u001b[0;34m(df_train, model_name, freeze_encoder, pretrained, plot)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# fit it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# plot history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1395\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "tokenizer = ConvBertTokenizer.from_pretrained(\"YituTech/conv-bert-base\")\n",
        "\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='convbert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p4_8CjcuiGr"
      },
      "outputs": [],
      "source": [
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for Convbert on SG1')\n",
        "print('10-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('10-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('10-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('10-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('10-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('10-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('10-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ConvBertTokenizer.from_pretrained(\"YituTech/conv-bert-base\")\n",
        "\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='convbert', \n",
        "                                                                                            freeze_encoder=False, pretrained=False)"
      ],
      "metadata": {
        "id": "vSaYquX4lvPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for Convbert on SG2')\n",
        "print('10-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('10-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('10-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('10-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('10-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('10-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('10-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "metadata": {
        "id": "DWwCimNfWXHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4zKOoHjVKKT"
      },
      "source": [
        "### ConvBERT + Distant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DQ5Ug7EUVTB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc21812-a48b-43ea-df0a-12022b3b0eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# load model layer weights from pretraining on distant dataset \n",
        "# compile model\n",
        "transfer_model = TFConvBertForSequenceClassification.from_pretrained(\"YituTech/conv-bert-base\")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "transfer_model.compile(optimizer=optimizer, loss=transfer_model.hf_compute_loss) \n",
        "\n",
        "transfer_model.load_weights('./checkpoints/roberta_final_checkpoint_news_headlines_USA')\n",
        "trained_model_layer = transfer_model.get_layer(index=0).get_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MIZDvX-3WE4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06552d5e-eb18-4541-93a2-993f5b88469d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Start fold 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 69s 1s/step - loss: 0.6216 - val_loss: 0.4772\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 39s 893ms/step - loss: 0.4283 - val_loss: 0.3609\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 39s 875ms/step - loss: 0.2628 - val_loss: 0.3514\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 38s 874ms/step - loss: 0.1474 - val_loss: 0.3885\n",
            "5/5 [==============================] - 1s 238ms/step - loss: 0.3514\n",
            "5/5 [==============================] - 6s 229ms/step\n",
            "### Start fold 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 70s 1s/step - loss: 0.6547 - val_loss: 0.5786\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 39s 876ms/step - loss: 0.5115 - val_loss: 0.4996\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 39s 877ms/step - loss: 0.3318 - val_loss: 0.4663\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 39s 876ms/step - loss: 0.2010 - val_loss: 0.5630\n",
            "5/5 [==============================] - 1s 243ms/step - loss: 0.4663\n",
            "5/5 [==============================] - 6s 239ms/step\n",
            "### Start fold 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 70s 1s/step - loss: 0.6519 - val_loss: 0.5329\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 39s 888ms/step - loss: 0.4456 - val_loss: 0.4271\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 39s 885ms/step - loss: 0.2869 - val_loss: 0.4615\n",
            "5/5 [==============================] - 2s 310ms/step - loss: 0.4271\n",
            "5/5 [==============================] - 6s 302ms/step\n",
            "### Start fold 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 70s 1s/step - loss: 0.6431 - val_loss: 0.5335\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 39s 879ms/step - loss: 0.4509 - val_loss: 0.4550\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 38s 875ms/step - loss: 0.2696 - val_loss: 0.4676\n",
            "5/5 [==============================] - 1s 248ms/step - loss: 0.4550\n",
            "5/5 [==============================] - 7s 241ms/step\n",
            "### Start fold 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 69s 1s/step - loss: 0.6311 - val_loss: 0.5981\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 38s 872ms/step - loss: 0.4206 - val_loss: 0.6143\n",
            "5/5 [==============================] - 1s 229ms/step - loss: 0.5981\n",
            "5/5 [==============================] - 7s 221ms/step\n",
            "### Start fold 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 72s 1s/step - loss: 0.6280 - val_loss: 0.5793\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 39s 884ms/step - loss: 0.4184 - val_loss: 0.5209\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 39s 876ms/step - loss: 0.2660 - val_loss: 0.4488\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 39s 880ms/step - loss: 0.1484 - val_loss: 0.4443\n",
            "Epoch 5/10\n",
            "44/44 [==============================] - 39s 877ms/step - loss: 0.0819 - val_loss: 0.6019\n",
            "5/5 [==============================] - 1s 240ms/step - loss: 0.4443\n",
            "5/5 [==============================] - 6s 233ms/step\n",
            "### Start fold 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 71s 1s/step - loss: 0.6298 - val_loss: 0.5387\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 39s 880ms/step - loss: 0.4302 - val_loss: 0.5214\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 39s 879ms/step - loss: 0.2819 - val_loss: 0.4963\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 39s 879ms/step - loss: 0.1555 - val_loss: 0.4997\n",
            "5/5 [==============================] - 1s 254ms/step - loss: 0.4963\n",
            "5/5 [==============================] - 6s 248ms/step\n",
            "### Start fold 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 70s 1s/step - loss: 0.6277 - val_loss: 0.5274\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 39s 884ms/step - loss: 0.4283 - val_loss: 0.4742\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 39s 878ms/step - loss: 0.3170 - val_loss: 0.4539\n",
            "Epoch 4/10\n",
            "44/44 [==============================] - 39s 878ms/step - loss: 0.1857 - val_loss: 0.6159\n",
            "5/5 [==============================] - 1s 244ms/step - loss: 0.4539\n",
            "5/5 [==============================] - 6s 238ms/step\n",
            "### Start fold 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 71s 1s/step - loss: 0.6396 - val_loss: 0.4773\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 39s 885ms/step - loss: 0.4579 - val_loss: 0.3786\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 39s 884ms/step - loss: 0.2769 - val_loss: 0.4173\n",
            "5/5 [==============================] - 2s 307ms/step - loss: 0.3786\n",
            "5/5 [==============================] - 6s 302ms/step\n",
            "### Start fold 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
            "\n",
            "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "44/44 [==============================] - 70s 1s/step - loss: 0.6561 - val_loss: 0.5694\n",
            "Epoch 2/10\n",
            "44/44 [==============================] - 39s 878ms/step - loss: 0.4792 - val_loss: 0.4409\n",
            "Epoch 3/10\n",
            "44/44 [==============================] - 39s 876ms/step - loss: 0.2973 - val_loss: 0.5116\n",
            "5/5 [==============================] - 1s 237ms/step - loss: 0.4409\n",
            "5/5 [==============================] - 8s 231ms/step\n"
          ]
        }
      ],
      "source": [
        "tokenizer = ConvBertTokenizer.from_pretrained(\"YituTech/conv-bert-base\")\n",
        "\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg1, model_name='convbert', \n",
        "                                                                                            freeze_encoder=False, pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Y6R_hKN_WHjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c566b4-f6df-4a7c-ec8f-5d9f21e5d35c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for Distant + Convbert on SG1\n",
            "10-Fold CV Loss: 0.4511946588754654\n",
            "10-Fold CV Accuracy: 0.8046837033933809\n",
            "10-Fold CV Precision: 0.835336794376578\n",
            "10-Fold CV Recall: 0.7559459459459459\n",
            "10-Fold CV F1 Score: 0.7893015910839514\n",
            "10-Fold CV Micro F1 Score: 0.8046837033933809\n",
            "10-Fold CV Weighted Macro F1 Score: 0.8033101509608663\n"
          ]
        }
      ],
      "source": [
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for Distant + Convbert on SG1')\n",
        "print('10-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('10-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('10-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('10-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('10-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('10-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('10-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ConvBertTokenizer.from_pretrained(\"YituTech/conv-bert-base\")\n",
        "\n",
        "# without distant signal pretraining\n",
        "val_loss, val_acc, val_prec, val_rec, val_f1, val_f1_micro, val_f1_wmacro = run_model_5fold(df_sg2, model_name='convbert', \n",
        "                                                                                            freeze_encoder=False, pretrained=True)"
      ],
      "metadata": {
        "id": "M1tTamVysLkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_cv = np.mean(val_loss)\n",
        "acc_cv = np.mean(val_acc)\n",
        "prec_cv = np.mean(val_prec)\n",
        "rec_cv = np.mean(val_rec)\n",
        "f1_cv = np.mean(val_f1)\n",
        "f1_micro_cv = np.mean(val_f1_micro)\n",
        "f1_wmacro_cv = np.mean(val_f1_wmacro)\n",
        "\n",
        "print('Results for Convbert+Distant on SG2')\n",
        "print('5-Fold CV Loss: {}'.format(loss_cv))\n",
        "print('5-Fold CV Accuracy: {}'.format(acc_cv))\n",
        "print('5-Fold CV Precision: {}'.format(prec_cv))\n",
        "print('5-Fold CV Recall: {}'.format(rec_cv))\n",
        "print('5-Fold CV F1 Score: {}'.format(f1_cv))\n",
        "print('5-Fold CV Micro F1 Score: {}'.format(f1_micro_cv))\n",
        "print('5-Fold CV Weighted Macro F1 Score: {}'.format(f1_wmacro_cv))"
      ],
      "metadata": {
        "id": "9xPW-lDHsSCH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z9t6QsLC5Oou",
        "XCW0oaxBzonB",
        "hRTchi-m2hYh",
        "FOMwgdo9H2Ax",
        "fVwXBK-03SbE",
        "hhsBtQ_S3tcC"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71da0de369e24868941591006463b496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48cdbd071c1745f88e8d40654cbc241e",
              "IPY_MODEL_df8b2230be3d4a9b81b927888316a1e3",
              "IPY_MODEL_83051d827a4f4148bfe0bef0336f68bf"
            ],
            "layout": "IPY_MODEL_874e139541bc4000a14e95bd2289bcc1"
          }
        },
        "48cdbd071c1745f88e8d40654cbc241e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2765f23d25de47ff84b7d0bca65f4117",
            "placeholder": "​",
            "style": "IPY_MODEL_cc3a2e39dae2484d82d93b3c15b2dbe9",
            "value": "Downloading: 100%"
          }
        },
        "df8b2230be3d4a9b81b927888316a1e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fcf02b91e0a49f38d525a7480345a9c",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69a7030824504d018c33ebfd119d4cfa",
            "value": 570
          }
        },
        "83051d827a4f4148bfe0bef0336f68bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eac21a01d944afaaa767f8bd101820e",
            "placeholder": "​",
            "style": "IPY_MODEL_a80d96f1dbb64d6586e86c23e15f9c8b",
            "value": " 570/570 [00:00&lt;00:00, 23.9kB/s]"
          }
        },
        "874e139541bc4000a14e95bd2289bcc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2765f23d25de47ff84b7d0bca65f4117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc3a2e39dae2484d82d93b3c15b2dbe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fcf02b91e0a49f38d525a7480345a9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69a7030824504d018c33ebfd119d4cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5eac21a01d944afaaa767f8bd101820e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a80d96f1dbb64d6586e86c23e15f9c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "552c8b1ae3244b68ae9a42230a2676c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae23131f15cf46fd87def09b9b6fdee8",
              "IPY_MODEL_a583f58c8cab4dd1843d9d651c0b9b6b",
              "IPY_MODEL_dfabb07514a145a49475b286591eca1c"
            ],
            "layout": "IPY_MODEL_7bb398f92cf94d48bbd6f54f20a978ff"
          }
        },
        "ae23131f15cf46fd87def09b9b6fdee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfc73d1c03124c6ca4bc43061b0c39c9",
            "placeholder": "​",
            "style": "IPY_MODEL_93d75ffa6e6a4fb7b199348cf3256b16",
            "value": "Downloading: 100%"
          }
        },
        "a583f58c8cab4dd1843d9d651c0b9b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_805f01ac63f84ad8a997b523285bfb48",
            "max": 536063208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7eda82fe227540e3a99b112ceb577c72",
            "value": 536063208
          }
        },
        "dfabb07514a145a49475b286591eca1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2458681341e54de0bc389a49592396a3",
            "placeholder": "​",
            "style": "IPY_MODEL_97e5c5ca6a2d4860984be0ddc52658f5",
            "value": " 536M/536M [00:43&lt;00:00, 31.7MB/s]"
          }
        },
        "7bb398f92cf94d48bbd6f54f20a978ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfc73d1c03124c6ca4bc43061b0c39c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93d75ffa6e6a4fb7b199348cf3256b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "805f01ac63f84ad8a997b523285bfb48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eda82fe227540e3a99b112ceb577c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2458681341e54de0bc389a49592396a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97e5c5ca6a2d4860984be0ddc52658f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0fba4513fb74431b40169f93342390c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae72c48ced54482f87375f225cc13e20",
              "IPY_MODEL_0acb7e17cb31493b99d056d20ca890e1",
              "IPY_MODEL_625b31b4e9cd47059e64c0a33b22d0b0"
            ],
            "layout": "IPY_MODEL_20dd5866e792419a97350ccc02e7be89"
          }
        },
        "ae72c48ced54482f87375f225cc13e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c389f74d4d44b89a3259b5f8aa8b9bb",
            "placeholder": "​",
            "style": "IPY_MODEL_cd0a4611904b46dd81182a60a7ebe8e2",
            "value": "Downloading: 100%"
          }
        },
        "0acb7e17cb31493b99d056d20ca890e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb84317e1c374e78a6638e3f26b2be4e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f6e37b8f5b04901b38ff476dbce9c6f",
            "value": 231508
          }
        },
        "625b31b4e9cd47059e64c0a33b22d0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66b1496875bf461592b5852190bd5cac",
            "placeholder": "​",
            "style": "IPY_MODEL_44dd78ad150f40e78db981d4e33407ff",
            "value": " 232k/232k [00:00&lt;00:00, 247kB/s]"
          }
        },
        "20dd5866e792419a97350ccc02e7be89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c389f74d4d44b89a3259b5f8aa8b9bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd0a4611904b46dd81182a60a7ebe8e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb84317e1c374e78a6638e3f26b2be4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f6e37b8f5b04901b38ff476dbce9c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66b1496875bf461592b5852190bd5cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44dd78ad150f40e78db981d4e33407ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7eb4c9ee024141b4b2152603e2af7d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5cf4cd659ba4cf3bcfe1b9cca8dd474",
              "IPY_MODEL_d963157df5424a208451c68245c7d53b",
              "IPY_MODEL_378fc88656d74c35979700a0ccaa0b90"
            ],
            "layout": "IPY_MODEL_5058a193680c45689eba2a50579de31e"
          }
        },
        "c5cf4cd659ba4cf3bcfe1b9cca8dd474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e998ea87949e4ca199427d290a5559a6",
            "placeholder": "​",
            "style": "IPY_MODEL_d19f75b256124e859a09b707d0193cd7",
            "value": "Downloading: 100%"
          }
        },
        "d963157df5424a208451c68245c7d53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05f9f62325564cf28adba723b914d57f",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e7f594899504498b7d71f7283bc46c9",
            "value": 28
          }
        },
        "378fc88656d74c35979700a0ccaa0b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb50d6a541674953a94eb6d48cbdb4dd",
            "placeholder": "​",
            "style": "IPY_MODEL_eb7e1cb2357a4de6a330e555c1cf2afc",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.20kB/s]"
          }
        },
        "5058a193680c45689eba2a50579de31e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e998ea87949e4ca199427d290a5559a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19f75b256124e859a09b707d0193cd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05f9f62325564cf28adba723b914d57f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e7f594899504498b7d71f7283bc46c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb50d6a541674953a94eb6d48cbdb4dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7e1cb2357a4de6a330e555c1cf2afc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92a3b3bd20f44f55ada21c9d3209ac53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adfee37b9ccf4ec3a97fc88736374d2a",
              "IPY_MODEL_53fc9077d1374d13aa9cd01756bfae00",
              "IPY_MODEL_1a8ea25d87644a16a6d2ed8f6cd58dd1"
            ],
            "layout": "IPY_MODEL_30690007f3284ca69071ee45b514f8cb"
          }
        },
        "adfee37b9ccf4ec3a97fc88736374d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7331fab31a1f41068a3e691a89dd520c",
            "placeholder": "​",
            "style": "IPY_MODEL_d3f53874b25b4b41830671c2b24e4a61",
            "value": "Downloading: 100%"
          }
        },
        "53fc9077d1374d13aa9cd01756bfae00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eec7b445ff84cefbf2cb9741ec8b84c",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc0a8f60a9d045d1a22e57f0f0939f58",
            "value": 481
          }
        },
        "1a8ea25d87644a16a6d2ed8f6cd58dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_206fdf9846774cc693f7788d289549b6",
            "placeholder": "​",
            "style": "IPY_MODEL_494ff39b996a49d8abd08dd3613903a2",
            "value": " 481/481 [00:00&lt;00:00, 19.0kB/s]"
          }
        },
        "30690007f3284ca69071ee45b514f8cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7331fab31a1f41068a3e691a89dd520c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3f53874b25b4b41830671c2b24e4a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eec7b445ff84cefbf2cb9741ec8b84c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc0a8f60a9d045d1a22e57f0f0939f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "206fdf9846774cc693f7788d289549b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494ff39b996a49d8abd08dd3613903a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fca27cf5a95c47f288b9449ee7d13608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9382706bb86049ea93ca2d5140d6bddf",
              "IPY_MODEL_61585c1d920f4a978ebd974a8060fb9a",
              "IPY_MODEL_b7e3f119310642dc85cc53300d565da0"
            ],
            "layout": "IPY_MODEL_9c53147a7124405c8d1dd96fc8181dda"
          }
        },
        "9382706bb86049ea93ca2d5140d6bddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b51648f73ef14dcb90088e64f3d6279c",
            "placeholder": "​",
            "style": "IPY_MODEL_13f26fc94b404cae8b4e0fb931bb07f8",
            "value": "Downloading: 100%"
          }
        },
        "61585c1d920f4a978ebd974a8060fb9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f20f193200264c22a80318ba78f48c43",
            "max": 657434796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15154a5731464b7b81b36e0309dc62a2",
            "value": 657434796
          }
        },
        "b7e3f119310642dc85cc53300d565da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2bb71996586492ab83ce3b0ace80774",
            "placeholder": "​",
            "style": "IPY_MODEL_fc6e96513f3d4aadae85f25c0d8667c0",
            "value": " 657M/657M [00:17&lt;00:00, 50.1MB/s]"
          }
        },
        "9c53147a7124405c8d1dd96fc8181dda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b51648f73ef14dcb90088e64f3d6279c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f26fc94b404cae8b4e0fb931bb07f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f20f193200264c22a80318ba78f48c43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15154a5731464b7b81b36e0309dc62a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2bb71996586492ab83ce3b0ace80774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc6e96513f3d4aadae85f25c0d8667c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e15af236028c4126a3bc8600d5a68023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d782b207dd44476af552f37eac3863f",
              "IPY_MODEL_8faebe4074eb43628ad6f0ecf509a9d1",
              "IPY_MODEL_0efa3632210c4f839101e72f2d489974"
            ],
            "layout": "IPY_MODEL_dcf5df4a1d0a4abf80860dde82ce6a91"
          }
        },
        "7d782b207dd44476af552f37eac3863f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eece09ce2e94668895b112e39593a57",
            "placeholder": "​",
            "style": "IPY_MODEL_45463475851e413c840493c72bfa6b49",
            "value": "Downloading: 100%"
          }
        },
        "8faebe4074eb43628ad6f0ecf509a9d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a29a30d58184098b7b56acfed7e6a0d",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41b6ff0a1e314d478a6d3525a6fd270c",
            "value": 898823
          }
        },
        "0efa3632210c4f839101e72f2d489974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50298701d21449d3b332c70ec4d09a0a",
            "placeholder": "​",
            "style": "IPY_MODEL_156b716f82674f94a59f9e638d978fef",
            "value": " 899k/899k [00:01&lt;00:00, 886kB/s]"
          }
        },
        "dcf5df4a1d0a4abf80860dde82ce6a91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eece09ce2e94668895b112e39593a57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45463475851e413c840493c72bfa6b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a29a30d58184098b7b56acfed7e6a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41b6ff0a1e314d478a6d3525a6fd270c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50298701d21449d3b332c70ec4d09a0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "156b716f82674f94a59f9e638d978fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37e262a4dc3f414ebdf30dd8278846b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efe90acb88574093bf871b94310c3c02",
              "IPY_MODEL_58dc0aabba4848c4bfa5ccf267b65c55",
              "IPY_MODEL_7f76af93c0ac49fc8952076c07dbcf71"
            ],
            "layout": "IPY_MODEL_43f32fda900346a284da7a00a38f620c"
          }
        },
        "efe90acb88574093bf871b94310c3c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7005abd674224c6bb330bfed7902366d",
            "placeholder": "​",
            "style": "IPY_MODEL_236c0601bb4147619ae2ea3410eed35c",
            "value": "Downloading: 100%"
          }
        },
        "58dc0aabba4848c4bfa5ccf267b65c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71337aa752f749489058a82d74b775ac",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11bc23ab3e1a45109b291fd16f333d8a",
            "value": 456318
          }
        },
        "7f76af93c0ac49fc8952076c07dbcf71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd55d1aae93b49cea3178ec7837043a0",
            "placeholder": "​",
            "style": "IPY_MODEL_b3bac04d6eb54f0e82f9ebb4512e6efa",
            "value": " 456k/456k [00:01&lt;00:00, 490kB/s]"
          }
        },
        "43f32fda900346a284da7a00a38f620c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7005abd674224c6bb330bfed7902366d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "236c0601bb4147619ae2ea3410eed35c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71337aa752f749489058a82d74b775ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11bc23ab3e1a45109b291fd16f333d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd55d1aae93b49cea3178ec7837043a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3bac04d6eb54f0e82f9ebb4512e6efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "211ba498c09c4721b8c1eeb740f18ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20cc25fa564747aa8adad5ec01a0e739",
              "IPY_MODEL_1a51639257d9411fb3b3cbe739e24ea7",
              "IPY_MODEL_e4f2939fe6f3442b88dc11577271f721"
            ],
            "layout": "IPY_MODEL_beb98d84e97641f7bbc6201185df72d9"
          }
        },
        "20cc25fa564747aa8adad5ec01a0e739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_080a6f5231d4452391702db3d4db9050",
            "placeholder": "​",
            "style": "IPY_MODEL_805b6a4f75b947ea8f118572498f43ba",
            "value": "Downloading: 100%"
          }
        },
        "1a51639257d9411fb3b3cbe739e24ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bf4a6c0bb8043d08d57cd7ad09c949d",
            "max": 266699,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6319d5fbedb54d6d8bdebef0503f9774",
            "value": 266699
          }
        },
        "e4f2939fe6f3442b88dc11577271f721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1962f05fd1e54c179c621104a2aef09a",
            "placeholder": "​",
            "style": "IPY_MODEL_f097848fddd34c12a03e4b016a82934f",
            "value": " 267k/267k [00:00&lt;00:00, 252kB/s]"
          }
        },
        "beb98d84e97641f7bbc6201185df72d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "080a6f5231d4452391702db3d4db9050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "805b6a4f75b947ea8f118572498f43ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bf4a6c0bb8043d08d57cd7ad09c949d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6319d5fbedb54d6d8bdebef0503f9774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1962f05fd1e54c179c621104a2aef09a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f097848fddd34c12a03e4b016a82934f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa575313d8054a98976fce778fa73753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4fdbdfd3870421caf5c8a7f6d317a98",
              "IPY_MODEL_541d398b3add4b6a913fb26670a41c14",
              "IPY_MODEL_ffcfbcf80bb943b899bbd50cca9393d0"
            ],
            "layout": "IPY_MODEL_50a3b349f2a64429abef6209a4b17933"
          }
        },
        "e4fdbdfd3870421caf5c8a7f6d317a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afdbb109545744c0991ef2a223235f9e",
            "placeholder": "​",
            "style": "IPY_MODEL_fcfe7c47a36d432ba553e22fc032c3c2",
            "value": "Downloading: 100%"
          }
        },
        "541d398b3add4b6a913fb26670a41c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beba76dd99724dd59b978834028d3c63",
            "max": 674,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_192390afc4f04226a25eedc4c9249b1f",
            "value": 674
          }
        },
        "ffcfbcf80bb943b899bbd50cca9393d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25bc3257506d44dd884f3de4557032d8",
            "placeholder": "​",
            "style": "IPY_MODEL_639f3277dd41436d8163a4420f40437c",
            "value": " 674/674 [00:00&lt;00:00, 37.0kB/s]"
          }
        },
        "50a3b349f2a64429abef6209a4b17933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afdbb109545744c0991ef2a223235f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcfe7c47a36d432ba553e22fc032c3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beba76dd99724dd59b978834028d3c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "192390afc4f04226a25eedc4c9249b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25bc3257506d44dd884f3de4557032d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "639f3277dd41436d8163a4420f40437c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5866004c14a34ddea40e031e7fd4861e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a09aebad8195440dac2bab5630730d9f",
              "IPY_MODEL_6ad0f08f28914f6ab5cbd1096681ceb5",
              "IPY_MODEL_c2ba956b2b634a7fa10103e8e0ceeb91"
            ],
            "layout": "IPY_MODEL_98ecaeb6f0e44095b963fc1a0be01e4c"
          }
        },
        "a09aebad8195440dac2bab5630730d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdc0085805a649f4822bac0fd4b3249d",
            "placeholder": "​",
            "style": "IPY_MODEL_7f8e8183988342dd93cc0fdd61869419",
            "value": "Downloading: 100%"
          }
        },
        "6ad0f08f28914f6ab5cbd1096681ceb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dde19e341c3f45b0806b6f1d0911c20c",
            "max": 423072408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6beeacbdd3a6469091546c2d3b44e47d",
            "value": 423072408
          }
        },
        "c2ba956b2b634a7fa10103e8e0ceeb91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_960a3ea43c5b40b4824cd1a1a9001d39",
            "placeholder": "​",
            "style": "IPY_MODEL_e5fe9b1161914854b6700cbbdc6e9384",
            "value": " 423M/423M [00:05&lt;00:00, 74.1MB/s]"
          }
        },
        "98ecaeb6f0e44095b963fc1a0be01e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdc0085805a649f4822bac0fd4b3249d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f8e8183988342dd93cc0fdd61869419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dde19e341c3f45b0806b6f1d0911c20c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6beeacbdd3a6469091546c2d3b44e47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "960a3ea43c5b40b4824cd1a1a9001d39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5fe9b1161914854b6700cbbdc6e9384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}